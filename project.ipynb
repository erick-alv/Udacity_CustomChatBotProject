{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "\n",
    "For this code, we use RAG with information on space missions between 2023 and 2024. We use the Wikipedia pages [2023 in spaceflight](https://en.wikipedia.org/wiki/2023_in_spaceflight) and [2024 in spaceflight](https://en.wikipedia.org/wiki/2024_in_spaceflight).\n",
    "\n",
    "\n",
    "This is useful to keep up-to-date information about space missions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Task: In the cells below, load your chosen dataset into a `pandas` dataframe with a column named `\"text\"`. This column should contain all of your text data, separated into at least 20 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6db6d-cc7f-42bd-8bee-3ee42841cc68",
   "metadata": {},
   "source": [
    "retrieving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69b83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def retrieve_wiki_to_file(page_title: str):\n",
    "    # Get the Wikipedia page; we do not set titles yet\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exlimit\": 1,\n",
    "        \"titles\": page_title,\n",
    "        \"explaintext\": 1,\n",
    "        \"formatversion\": 2,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    resp = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params)\n",
    "    response_dict = resp.json()\n",
    "    with open(page_title + \".json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(response_dict,f)\n",
    "\n",
    "retrieve_wiki_to_file(\"2023_in_spaceflight\")\n",
    "retrieve_wiki_to_file(\"2024_in_spaceflight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a595980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading json\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "json_filename = \"2023_in_spaceflight.json\"\n",
    "sp_2023_json = load_json(json_filename)\n",
    "json_filename = \"2024_in_spaceflight.json\"\n",
    "sp_2024_json = load_json(json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560e04da-f1cb-4697-948e-09c628191061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning data\n",
    "def cleanup_data(original_data):\n",
    "    cleaned_data = [item for item in original_data if len(item) > 0]\n",
    "    cleaned_data = [item for item in cleaned_data if not (item.startswith(\"=\") and item.endswith(\"=\"))]\n",
    "    return cleaned_data\n",
    "sp_2024_data = cleanup_data(sp_2024_json['query']['pages'][0]['extract'].split(\"\\n\"))\n",
    "sp_2023_data = cleanup_data(sp_2023_json['query']['pages'][0]['extract'].split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853eab47-ede3-42c4-9840-98559251b8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The year 2023 saw rapid growth and significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In terms of national-level scientific space mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two crewed space stations, the International S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This year also saw the first time citizens of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>European Space Agency's (ESA) Euclid satellite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The year 2023 saw rapid growth and significant...\n",
       "1  In terms of national-level scientific space mi...\n",
       "2  Two crewed space stations, the International S...\n",
       "3  This year also saw the first time citizens of ...\n",
       "4  European Space Agency's (ESA) Euclid satellite..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = sp_2023_data + sp_2024_data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion\n",
    "\n",
    "Task: In the cells below, compose a custom query using your chosen dataset and retrieve results from an OpenAI `Completion` model. You may copy and paste any useful code from the course materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0b196a-d242-4d34-946d-93ca7ac5f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"#TODO set key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582f0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing openai and setting the client\n",
    "import openai\n",
    "import os\n",
    "client = openai.OpenAI(\n",
    "    base_url=f\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6e1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst for openai\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\"\n",
    "GPT_MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87349dd8-43a6-449a-961f-4945d5345fcd",
   "metadata": {},
   "source": [
    "Creating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f2dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the embeddings as in casestudy case\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    # Send text data to OpenAI model to get embeddings\n",
    "    response = client.embeddings.create(\n",
    "        input=df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "        model=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # Add embeddings to list\n",
    "    embeddings.extend([data.embedding for data in response.data])\n",
    "\n",
    "# Add embeddings list to dataframe\n",
    "df[\"embeddings\"] = embeddings\n",
    "#saving embeddings\n",
    "df.to_csv(\"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading if needed\n",
    "#import pandas as pd\n",
    "#from ast import literal_eval\n",
    "#df = pd.read_csv(\"embeddings.csv\")\n",
    "#df[\"embeddings\"] = df[\"embeddings\"].apply(literal_eval)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89c925-d6a3-4037-951d-a2f8e908c154",
   "metadata": {},
   "source": [
    "Function for sorting rows by similarity with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b5f1d6-81ea-4e4b-8ce6-5eec987ffd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    embedding_response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    return embedding_response.data[0].embedding\n",
    "    \n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "    \"\"\"\n",
    "    Function that takes in a question string and a dataframe containing\n",
    "    rows of text and associated embeddings, and returns that dataframe\n",
    "    sorted from least to most relevant for that question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get embeddings for the question text\n",
    "    question = question.replace(\"\\n\", \" \")\n",
    "    question = question.replace(\"  \", \" \")\n",
    "    question_embeddings = get_embedding(question)\n",
    "    \n",
    "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "    # the cosine distances between each row's embeddings and the\n",
    "    # embeddings of the question\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = df_copy[\"embeddings\"].apply(\n",
    "        lambda el_embeddings: spatial.distance.cosine(question_embeddings, el_embeddings)\n",
    "    )\n",
    "    \n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    # (shorter distance = more relevant so we sort in ascending order)\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9c595-0422-4584-8da2-5fa484d4cb99",
   "metadata": {},
   "source": [
    "Function for creating the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11bf49a6-c7ca-46f2-85d7-d673b8c6f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import copy\n",
    "\n",
    "PROMPT_MSGS_TEMPLATE = [\n",
    "    {\n",
    "        \"role\":\"system\", \n",
    "        \"content\":\"\"\"\n",
    "You are a helpful assistant that will answer the question of the user with the information provided in the context. If if the question can't be answered based on the context, say 'I don't know'.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"\"\"\n",
    "Context:\n",
    "{}\n",
    "_____\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"\"\"\n",
    "Question:\n",
    "{}\"\"\"\n",
    "     }\n",
    "]\n",
    "\n",
    "def create_prompt_msgs(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.encoding_for_model(GPT_MODEL_NAME)\n",
    "    context_separator = \"\\n\\n###\\n\\n\"\n",
    "    tokens_per_separator = len(tokenizer.encode(context_separator))\n",
    "\n",
    "    \n",
    "    #counting tokens for structure of message\n",
    "    tokens_per_message = 4 #the new api uses some tokens to structure each message\n",
    "    current_token_count = tokens_per_message*len(PROMPT_MSGS_TEMPLATE)\n",
    "    current_token_count += 3 #the new api adds this number of tokens to the answer\n",
    "    for msg_el in PROMPT_MSGS_TEMPLATE:\n",
    "        current_token_count += len(tokenizer.encode(msg_el[\"content\"].format(\"\")))\n",
    "    current_token_count += len(tokenizer.encode(question))\n",
    "\n",
    "    #putting the messages that fit in the context\n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        \n",
    "        if text_token_count + current_token_count <= max_token_count:\n",
    "            current_token_count += text_token_count\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        if current_token_count + tokens_per_separator <= max_token_count:\n",
    "            current_token_count += tokens_per_separator\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #creating the message\n",
    "    prompt_msgs = copy.deepcopy(PROMPT_MSGS_TEMPLATE)\n",
    "    prompt_msgs[1][\"content\"] = prompt_msgs[1][\"content\"].format(context_separator.join(context))\n",
    "    prompt_msgs[2][\"content\"] = prompt_msgs[2][\"content\"].format(question)\n",
    "    return prompt_msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fd536-c09b-426a-bfd8-a4d02d246b42",
   "metadata": {},
   "source": [
    "Function for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18313b39-fb30-4c0b-990e-08fce2450fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=1800, max_answer_tokens=150\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "    \n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_msgs = create_prompt_msgs(question, df, max_prompt_tokens)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=GPT_MODEL_NAME,\n",
    "            messages=prompt_msgs,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration\n",
    "\n",
    "Task: In the cells below, demonstrate the performance of your custom query using at least 2 questions. For each question, show the answer from a basic `Completion` model query as well as the answer from your custom query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1273a11c-4fb6-4f5d-b5d4-5a862f522ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for retrieving simple answer\n",
    "def simple_answer(question, max_answer_tokens=150):\n",
    "    \"\"\"\n",
    "    Given a question and max of answer tokesn returns the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "    \n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "    simple_prompt_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that responds to the questions of the user.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=GPT_MODEL_NAME,\n",
    "            messages=simple_prompt_messages,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7a093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingenuity_question = \"When did the Ingenuity helicopter make its last flight on Mars?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb06508-b7f1-406a-bc6e-01aa0f6f647d",
   "metadata": {},
   "source": [
    "#### orignal answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5217b41a-a7c1-4160-ace2-4db33fb5bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last knowledge update in October 2023, the Ingenuity helicopter on Mars made its last recorded flight on September 6, 2022. However, please verify with the latest sources for any updates beyond that date.\n"
     ]
    }
   ],
   "source": [
    "simple_ingenuity_answer = simple_answer(ingenuity_question)\n",
    "print(simple_ingenuity_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920f84b-cb2a-4675-a27e-46a534e77dd7",
   "metadata": {},
   "source": [
    "#### custom answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f7deab0-7694-4e72-97c3-816ef4b0021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ingenuity helicopter made its last flight on Mars on January 18, 2024.\n"
     ]
    }
   ],
   "source": [
    "custom_ingenuity_answer = answer_question(ingenuity_question, df)\n",
    "print(custom_ingenuity_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f646989",
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_question=\"Which is the heaviest geostationary satellite ever launched, and who launched it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ae875-1f5c-4571-99c8-2f33a9c2a7d7",
   "metadata": {},
   "source": [
    "#### orignal answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "354b5f30-a56d-4eb1-8402-74cf55195e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of October 2023, the heaviest geostationary satellite ever launched is the GSAT-19, which was launched by the Indian Space Research Organisation (ISRO) on June 5, 2017. GSAT-19 weighs approximately 3,136 kilograms (6,910 pounds).\n"
     ]
    }
   ],
   "source": [
    "simple_satellite_answer=simple_answer(satellite_question)\n",
    "print(simple_satellite_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daed7957-f62b-4daa-b264-dfb70c190c85",
   "metadata": {},
   "source": [
    "#### custom answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d5e873-ac3b-423e-9a70-cf60d9d02ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heaviest geostationary satellite ever launched is the Jupiter-3 (EchoStar-24), and it was launched by a Falcon Heavy rocket.\n"
     ]
    }
   ],
   "source": [
    "custom_satellite_answer = answer_question(satellite_question, df)\n",
    "print(custom_satellite_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
